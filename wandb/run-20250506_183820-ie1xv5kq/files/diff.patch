diff --git a/agents/__init__.py b/agents/__init__.py
index a303373..ff2dc5e 100644
--- a/agents/__init__.py
+++ b/agents/__init__.py
@@ -2,6 +2,7 @@ from .drnd_agent import DRNDAgent
 from .epsilon_greedy_agent import EpsilonGreedyAgent
 from .rnd_agent import RNDAgent
 from .drnd_agent import DRNDAgent
+from .drnd_ppo_agent import DRNDPPOAgent
 
 def make_agent(agent_type: str, envs, config: dict):
     if agent_type == "epsilon_greedy":
@@ -10,5 +11,7 @@ def make_agent(agent_type: str, envs, config: dict):
         return RNDAgent(envs, config)
     elif agent_type == "drnd":
         return DRNDAgent(envs, config)
+    elif agent_type == "drnd_ppo":
+        return DRNDPPOAgent(envs, config)
     else:
         raise ValueError(f"Unknown agent type: {agent_type}")
\ No newline at end of file
diff --git a/agents/__pycache__/__init__.cpython-310.pyc b/agents/__pycache__/__init__.cpython-310.pyc
index cca36a7..aa48f39 100644
Binary files a/agents/__pycache__/__init__.cpython-310.pyc and b/agents/__pycache__/__init__.cpython-310.pyc differ
diff --git a/agents/__pycache__/drnd_agent.cpython-310.pyc b/agents/__pycache__/drnd_agent.cpython-310.pyc
index 10dd961..cd8718a 100644
Binary files a/agents/__pycache__/drnd_agent.cpython-310.pyc and b/agents/__pycache__/drnd_agent.cpython-310.pyc differ
diff --git a/agents/__pycache__/epsilon_greedy_agent.cpython-310.pyc b/agents/__pycache__/epsilon_greedy_agent.cpython-310.pyc
index 14e72bc..394701c 100644
Binary files a/agents/__pycache__/epsilon_greedy_agent.cpython-310.pyc and b/agents/__pycache__/epsilon_greedy_agent.cpython-310.pyc differ
diff --git a/agents/__pycache__/rnd_agent.cpython-310.pyc b/agents/__pycache__/rnd_agent.cpython-310.pyc
index a52fcc3..3ab5ee9 100644
Binary files a/agents/__pycache__/rnd_agent.cpython-310.pyc and b/agents/__pycache__/rnd_agent.cpython-310.pyc differ
diff --git a/agents/drnd_agent.py b/agents/drnd_agent.py
index c339167..8d4df20 100644
--- a/agents/drnd_agent.py
+++ b/agents/drnd_agent.py
@@ -58,8 +58,8 @@ class DRNDAgent:
 
         return np.array([action])
 
-    def record_step(self, obs, next_obs, actions, rewards, dones, infos):
-        intrinsic_reward = self.drnd.compute_intrinsic_reward(obs)
+    def record_step(self, obs, next_obs, actions, rewards, dones, infos, global_step, max_steps):
+        intrinsic_reward = self.drnd.compute_intrinsic_reward(obs, global_step, max_steps)
         total_reward = rewards + self.config["intrinsic_reward_scale"] * intrinsic_reward
         self.rb.add(obs, next_obs, actions, total_reward, dones, infos)
 
diff --git a/agents/drnd_ppo_agent.py b/agents/drnd_ppo_agent.py
new file mode 100644
index 0000000..ea1a154
--- /dev/null
+++ b/agents/drnd_ppo_agent.py
@@ -0,0 +1,139 @@
+from functools import partial
+import jax
+import jax.numpy as jnp
+import numpy as np
+import optax
+import flax
+from flax.training.train_state import TrainState
+import time
+import os
+
+from networks.drnd_ppo_network import DRNDActorCritic
+from exploration.drnd_ppo import DRNDPredictor
+from utils.replay_buffer import ReplayBuffer
+
+class DRNDPPOAgent:
+    def __init__(self, envs, config):
+        self.envs = envs
+        self.config = config
+        self.obs_shape = envs.single_observation_space.shape
+        self.action_dim = envs.single_action_space.n
+
+        # Random key
+        self.key = jax.random.PRNGKey(config["seed"])
+
+        # Networks
+        self.actor_critic = DRNDActorCritic(self.obs_shape, self.action_dim)
+        dummy_obs = jnp.zeros((1, *self.obs_shape))
+        init_vars = self.actor_critic.init(self.key, dummy_obs)
+
+        self.policy_state = TrainState.create(
+            apply_fn=self.actor_critic.apply,
+            params=init_vars["params"],
+            tx=optax.chain(
+                optax.clip_by_global_norm(config["max_grad_norm"]),
+                optax.adam(config["learning_rate"]),
+            ),
+        )
+
+        self.drnd = DRNDPredictor(self.obs_shape, config)
+
+        # Replay Buffer
+        self.rb = ReplayBuffer(
+            buffer_size=config["buffer_size"],
+            observation_space=envs.single_observation_space,
+            action_space=envs.single_action_space,
+        )
+
+        self.gamma = config["gamma"]
+        self.lam = config["gae_lambda"]
+        self.ent_coef = config["ent_coef"]
+        self.clip_eps = config["clip_eps"]
+        self.update_epochs = config["update_epochs"]
+        self.batch_size = config["batch_size"]
+        self.update_proportion = config["update_proportion"]
+
+        self.log_info = {}
+
+    @partial(jax.jit, static_argnums=0)
+    def _select_action_jit(self, params, obs, key):
+        policy_logits, value_ext, value_int = self.actor_critic.apply({"params": params}, obs)
+        probs = jax.nn.softmax(policy_logits)
+        action = jax.random.categorical(key, policy_logits)
+        return action, value_ext.squeeze(), value_int.squeeze(), probs
+
+    def select_action(self, obs, global_step):
+        self.key, subkey = jax.random.split(self.key)
+        action, value_ext, value_int, probs = self._select_action_jit(self.policy_state.params, obs, subkey)
+        action = jax.device_get(action)
+        return np.array([action]), value_ext, value_int, probs
+
+    def record_step(self, obs, next_obs, actions, rewards, dones, infos):
+        intrinsic_reward = self.drnd.compute_intrinsic_reward(next_obs)
+        total_reward = rewards + self.config["intrinsic_reward_scale"] * intrinsic_reward
+        self.rb.add(obs, next_obs, actions, total_reward, dones, infos)
+
+    def log_metrics(self, global_step, start_time):
+        sps = int(global_step / (time.time() - start_time))
+        self.log_info["charts/SPS"] = sps
+        return self.log_info
+
+    def update_target_network(self):
+        # PPO doesn't have a target network
+        pass
+
+    def save(self, path):
+        os.makedirs(os.path.dirname(path), exist_ok=True)
+        state = {
+            "policy_params": self.policy_state.params,
+            "drnd_predictor_params": self.drnd.train_state.params,
+        }
+        with open(path, "wb") as f:
+            f.write(flax.serialization.to_bytes(state))
+
+    def train_step(self, global_step):
+        if not self.rb.can_sample(self.batch_size):
+            return
+
+        batch = self.rb.sample(self.batch_size)
+        obs, next_obs, actions, rewards, dones = (
+            batch.observations,
+            batch.next_observations,
+            batch.actions,
+            batch.rewards.squeeze(),
+            batch.dones.squeeze(),
+        )
+
+        self.policy_state, policy_loss = self._ppo_update(obs, actions, rewards, dones)
+
+        # Update DRND predictor
+        self.drnd.update(next_obs)
+
+        self.log_info = {
+            "losses/policy_loss": float(policy_loss),
+        }
+
+    @partial(jax.jit, static_argnums=0)
+    def _ppo_update(self, obs, actions, rewards, dones):
+        def loss_fn(params):
+            logits, value_ext, value_int = self.actor_critic.apply({"params": params}, obs)
+            log_probs = jax.nn.log_softmax(logits)
+            action_log_probs = jnp.take_along_axis(log_probs, actions, axis=1).squeeze()
+
+            entropy_loss = -jnp.mean(jax.scipy.special.entr(jax.nn.softmax(logits)))
+            value_loss = jnp.mean((value_ext.squeeze() - rewards) ** 2)
+
+            actor_loss = -jnp.mean(action_log_probs * rewards)
+
+            total_loss = actor_loss + 0.5 * value_loss - self.ent_coef * entropy_loss
+            return total_loss
+
+        loss, grads = jax.value_and_grad(loss_fn)(self.policy_state.params)
+        updates, new_opt_state = self.policy_state.tx.update(grads, self.policy_state.opt_state)
+        new_params = optax.apply_updates(self.policy_state.params, updates)
+        new_policy_state = self.policy_state.replace(
+            step=self.policy_state.step + 1,
+            params=new_params,
+            opt_state=new_opt_state
+        )
+        return new_policy_state, loss
diff --git a/agents/epsilon_greedy_agent.py b/agents/epsilon_greedy_agent.py
index e5ee468..3a3f25d 100644
--- a/agents/epsilon_greedy_agent.py
+++ b/agents/epsilon_greedy_agent.py
@@ -85,7 +85,7 @@ class EpsilonGreedyAgent:
             )
         )
 
-    def record_step(self, obs, next_obs, actions, rewards, terminations, infos):
+    def record_step(self, obs, next_obs, actions, rewards, terminations, infos, global_step, max_steps):
         real_next_obs = next_obs.copy()
         for idx, trunc in enumerate(terminations):
             if trunc:
diff --git a/agents/rnd_agent.py b/agents/rnd_agent.py
index 950ce06..96f92dc 100644
--- a/agents/rnd_agent.py
+++ b/agents/rnd_agent.py
@@ -60,7 +60,7 @@ class RNDAgent:
 
         return np.array([action])
 
-    def record_step(self, obs, next_obs, actions, rewards, dones, infos):
+    def record_step(self, obs, next_obs, actions, rewards, dones, infos, global_step, max_steps):
         # Compute intrinsic reward
         intrinsic_reward = self.rnd.compute_intrinsic_reward(obs)
         total_reward = rewards + self.config["intrinsic_coef"] * intrinsic_reward
diff --git a/configs/drnd_ppo.yaml b/configs/drnd_ppo.yaml
new file mode 100644
index 0000000..558509f
--- /dev/null
+++ b/configs/drnd_ppo.yaml
@@ -0,0 +1,33 @@
+env_id: "CartPole-v1"
+env_name: "CartPole-v1"
+exp_name: "drnd_ppo_experiment"
+num_envs: 1
+seed: 42
+track: true
+capture_video: true
+save_model: false
+wandb_project_name: "nanodqn"
+agent_type: drnd_ppo
+
+# Training parameters
+total_timesteps: 100000
+learning_rate: 0.0003
+gamma: 0.99
+buffer_size: 100000
+batch_size: 64
+exploration_fraction: 0.1
+exploration_final_eps: 0.01
+
+# DRND-PPO-specific parameters
+predictor_hidden_dim: 512
+target_hidden_dim: 512
+intrinsic_reward_scale: 1.0
+num_target_networks: 5
+alpha: 0.95
+
+# PPO-specific parameters
+ppo_epochs: 4
+ppo_batch_size: 64
+ppo_clip_eps: 0.2
+ent_coef: 0.01
+max_grad_norm: 0.5
\ No newline at end of file
diff --git a/exploration/__pycache__/drnd.cpython-310.pyc b/exploration/__pycache__/drnd.cpython-310.pyc
index 48f980e..f9ce299 100644
Binary files a/exploration/__pycache__/drnd.cpython-310.pyc and b/exploration/__pycache__/drnd.cpython-310.pyc differ
diff --git a/exploration/drnd.py b/exploration/drnd.py
index 3275d88..820b307 100644
--- a/exploration/drnd.py
+++ b/exploration/drnd.py
@@ -24,16 +24,23 @@ class DRND:
             tx=self.optimizer
         )
 
-    def compute_intrinsic_reward(self, obs):
+    def compute_intrinsic_reward(self, obs, global_step, max_steps):
         mean_pred, std_pred = self.predictor.apply(self.train_state.params, obs)
         mean_target, std_target = self.target.apply(self.target_params, obs)
 
-        # Compute Mahalanobis distance
-        diff = mean_pred - mean_target
-        intrinsic_reward = jnp.sum((diff ** 2) / (std_pred ** 2 + 1e-8), axis=-1)
+        b1 = jnp.sum((mean_pred - mean_target) ** 2 / (std_pred ** 2 + 1e-8), axis=-1)
+
+        b2 = jnp.sum((std_pred ** 2 - std_target ** 2) / (std_pred ** 2 + 1e-8), axis=-1)
+
+        alpha = self.dynamic_alpha(global_step, max_steps)  # TODO: Could make it dynamic
+        intrinsic_reward = alpha * b1 + (1 - alpha) * b2
 
         return intrinsic_reward
 
+    def dynamic_alpha(self, global_step, max_steps):
+        # Decrease alpha from 1 to 0 as training progresses
+        return max(0, 1 - global_step / max_steps)
+
     @partial(jax.jit, static_argnums=0)
     def update_step(self, train_state, obs):
         def loss_fn(params):
diff --git a/exploration/drnd_ppo.py b/exploration/drnd_ppo.py
new file mode 100644
index 0000000..acac2aa
--- /dev/null
+++ b/exploration/drnd_ppo.py
@@ -0,0 +1,82 @@
+import flax.linen as nn
+import jax
+import jax.numpy as jnp
+import optax
+from flax.training import train_state
+
+class DRNDPredictor(nn.Module):
+    hidden_dim: int = 512
+
+    @nn.compact
+    def __call__(self, x):
+        # Flatten if necessary
+        if len(x.shape) > 2:
+            x = x.reshape((x.shape[0], -1))
+        x = nn.Dense(self.hidden_dim)(x)
+        x = nn.relu(x)
+        x = nn.Dense(self.hidden_dim)(x)
+        x = nn.relu(x)
+        x = nn.Dense(self.hidden_dim)(x)
+        return x
+
+class DRNDTarget(nn.Module):
+    hidden_dim: int = 512
+
+    @nn.compact
+    def __call__(self, x):
+        if len(x.shape) > 2:
+            x = x.reshape((x.shape[0], -1))
+        x = nn.Dense(self.hidden_dim)(x)
+        return x
+
+class DRNDModule:
+    def __init__(self, obs_shape, config):
+        self.predictor = DRNDPredictor(hidden_dim=config["predictor_hidden_dim"])
+        self.target_networks = [
+            DRNDTarget(hidden_dim=config["target_hidden_dim"])
+            for _ in range(config["num_target_networks"])
+        ]
+
+        dummy_input = jnp.zeros((1, *obs_shape))
+
+        self.predictor_params = self.predictor.init(jax.random.PRNGKey(0), dummy_input)
+        self.target_params = [t.init(jax.random.PRNGKey(i+1), dummy_input) for i, t in enumerate(self.target_networks)]
+
+        self.optimizer = optax.adam(config["learning_rate"])
+        self.train_state = train_state.TrainState.create(
+            apply_fn=self.predictor.apply,
+            params=self.predictor_params,
+            tx=self.optimizer,
+        )
+        self.config = config
+
+    def compute_intrinsic_reward(self, obs):
+        predict_feature = self.predictor.apply(self.train_state.params, obs)
+
+        # Stack all target outputs
+        target_features = jnp.stack([t.apply(params, obs) for t, params in zip(self.target_networks, self.target_params)], axis=0)
+
+        # Calculate the Mahalanobis-like distance
+        mu = jnp.mean(target_features, axis=0)
+        B2 = jnp.mean(target_features ** 2, axis=0)
+
+        alpha = self.config.get("alpha", 0.95)
+
+        intrinsic_reward = alpha * jnp.sum((predict_feature - mu) ** 2, axis=-1) \
+                         + (1 - alpha) * jnp.sum(jnp.sqrt(jnp.clip(jnp.abs(predict_feature ** 2 - mu ** 2) / (B2 - mu ** 2 + 1e-8), 1e-6, 1)), axis=-1)
+        return intrinsic_reward
+
+    def update(self, obs):
+        def loss_fn(params):
+            predict_feature = self.predictor.apply(params, obs)
+
+            # Randomly pick target networks for each sample
+            idx = jax.random.randint(jax.random.PRNGKey(0), shape=(obs.shape[0],), minval=0, maxval=len(self.target_networks))
+            selected_targets = jnp.stack([self.target_networks[i].apply(self.target_params[i], obs[i:i+1])[0] for i in idx])
+
+            # Forward loss (mean squared error)
+            loss = jnp.mean((predict_feature - selected_targets) ** 2)
+            return loss
+
+        grads = jax.grad(loss_fn)(self.train_state.params)
+        self.train_state = self.train_state.apply_gradients(grads=grads)
diff --git a/main.py b/main.py
index e1d16dc..01d3c2a 100644
--- a/main.py
+++ b/main.py
@@ -19,7 +19,7 @@ import tyro
 from flax.training.train_state import TrainState
 from tqdm.auto import tqdm
 
-with open("configs/drnd.yaml", "r") as f:
+with open("configs/drnd_ppo.yaml", "r") as f:
     config = yaml.safe_load(f)
 
 
@@ -144,7 +144,7 @@ if __name__ == "__main__":
             progress_bar.set_description_str(f"Reward: {float(last_mean_rs):.2f}")
 
         # save data to reply buffer; handle `final_observation`
-        agent.record_step(obs, next_obs, actions, rewards, terminations, infos)
+        agent.record_step(obs, next_obs, actions, rewards, terminations, infos, global_step, args.total_timesteps)
         agent.train_step(global_step)
 
         # CRUCIAL step easy to overlook, moving to the new observations
diff --git a/networks/drnd_ppo_network.py b/networks/drnd_ppo_network.py
new file mode 100644
index 0000000..bf214a3
--- /dev/null
+++ b/networks/drnd_ppo_network.py
@@ -0,0 +1,26 @@
+import flax.linen as nn
+
+class DRNDActorCritic(nn.Module):
+    obs_shape: tuple
+    action_dim: int
+
+    @nn.compact
+    def __call__(self, x):
+        # Flatten if necessary
+        if len(x.shape) > 2:
+            x = x.reshape((x.shape[0], -1))
+
+        # Shared hidden layers
+        x = nn.Dense(256)(x)
+        x = nn.relu(x)
+        x = nn.Dense(256)(x)
+        x = nn.relu(x)
+
+        # Actor head
+        logits = nn.Dense(self.action_dim)(x)
+
+        # Critic heads
+        value_ext = nn.Dense(1)(x)
+        value_int = nn.Dense(1)(x)
+
+        return logits, value_ext, value_int
diff --git a/run_all.py b/run_all.py
new file mode 100644
index 0000000..23af57a
--- /dev/null
+++ b/run_all.py
@@ -0,0 +1,16 @@
+import os
+import subprocess
+
+# Define environments and algorithms
+envs = ["CartPole-v1", "LunarLander-v2", "MountainCar-v0"]
+agents = ["epsilon_greedy", "rnd"]
+
+# Base command
+base_command = "python main.py"
+
+# Loop over everything
+for env in envs:
+    for agent in agents:
+        print(f"Running {agent} on {env}...")
+        command = f"{base_command} --env_id {env} --agent_type {agent}"
+        subprocess.run(command, shell=True)
\ No newline at end of file
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 79d1e90..2d42b17 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250505_161921-ffkcxun8/logs/debug-internal.log
\ No newline at end of file
+run-20250506_183820-ie1xv5kq/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 20acf0b..d5a8648 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250505_161921-ffkcxun8/logs/debug.log
\ No newline at end of file
+run-20250506_183820-ie1xv5kq/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 67286f8..91ba20a 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250505_161921-ffkcxun8
\ No newline at end of file
+run-20250506_183820-ie1xv5kq
\ No newline at end of file
